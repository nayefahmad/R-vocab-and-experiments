---
title: "Gower's dissimilarity measure for mixed numeric/categorical data"
author: "Nayef Ahmad"
date: "September 27, 2018"
output: 
    html_document: 
        code_folding: hide 
        toc: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(knitr.table.format = "html")


library("kableExtra")
library("cluster")
library("ISLR")
library("dplyr")
library("cluster")
library("stringr")


# shortcuts: 
# > ctrl + alt + i ==> new code chunk
# > ctrl + shift + k ==> knit doc 
# \  ==> line break (include 2 spaces after)
```


## Motivation for Gower's dissimilarity measure
Quite often, data analysis involves comparing different entities using a set of measurements on several variables. For example, patients in a hospital can be compared based on their length of stay in the hospital, their diagnoses, the number of medications they are taking, etc. 

I was recently looking into a problem that called for clustering patients into groups. **K-means clustering** - possibly the most widely-known clustering algorithm - only works when all variables are numeric. However, we often want to cluster observations on both numeric and categorical variables. For relatively small datasets, this can be done with **hierarchical clustering** methods using Gower's similarity coefficient. For larger datasets, the computational costs of hierarchical clustering are too large, and an alternative clustering method such as **k-prototypes** should be considered.^[Huang, Zhexue. Extensions to the k-Means algorithm for clustering large datasets with categorical values. *Data Mining and Knowledge Discovery*, 1998.] 

For details on the mechanics of Gower's measure, please see Gower, 1971.^[Gower, J.C. A general coefficient of similarity and some of its properties. *Biometrics*, 1971.] Here's the gist of it: 

To compare 2 people, A and B, on a variable X1, you first have to check whether a comparison is possible - i.e. measurements of X1 exist for both person A and person B. If a comparison is possible, then you assign a score for how similar these 2 people are. For example, if you want to compare Amy and Bob in terms of the variable "Nationality", then you could assign a value of 1 if they are both American. Call this *X1_similarity*.

The overall similarity score between Amy and Bob is the sum of all the individual variable similarities (*X1_similarity* + *X2_similarity* + ...) divided by the total possible comparisons (the number of variables for which data exists for both Amy and Bob). 

\  
\  
\  


## Numeric-only data: Comparing Gower's measure with Euclidean distances
Let's examine the Auto dataset from the package ISLR. Here's a sample of rows to show what it looks like: 

```{r message = FALSE}
df1.auto <- ISLR::Auto %>% 
    
    # recode origin as a factor 
    mutate(origin = case_when(
        origin == 1 ~ "American",
        origin == 2 ~ "European",
        origin == 3 ~ "Japanese"
    ) %>% as.factor())


# str(df1.auto)
# summary(df1.auto)
df1.auto %>% 
      sample_n(15) %>% 
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

```


\  
\  
\  

For simplicity, let's focus on a small subset of the data: 

```{r}
set.seed(3)

df2.auto.subset <- df1.auto %>% 
    select(name,
           mpg, 
           cylinders, 
           displacement) %>%  
    sample_n(3)

# print: 
df2.auto.subset %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

```

\  
\  
\  

Now we'll compare Gower's dissimilarity measure with Euclidean distances: 

\  
\  
\  

#### Euclidean distances: 

```{r}
# create matrix object for convenience: 
m1.auto.numeric <- select(df2.auto.subset, 
                              -name) %>% 
    as.matrix()

rownames(m1.auto.numeric) <- df2.auto.subset %>% pull(name)

m1.euclid.dis <- dist(m1.auto.numeric,
                       method = "euclidean")

m1.euclid.dis
# str(m1.euclid.dis)

```

So, in terms of similarity: 

* the plymouth valiant and amc ambassador are most similar
* the audi is more similar to the plymouth than to the amc 

Let's compare this with the results from Gower's dissimilarity: 

\  
\  
\  

#### Gower's dissimilarity: 

```{r}
m1.gower.dis <- daisy(m1.auto.numeric, 
                      metric = "gower")

m1.gower.dis

```

Same conclusion: 

* the plymouth valiant and amc ambassador are most similar
* the audi is more similar to the plymouth than to the amc

\  
\  
\  

## Mixed numeric/categorical data
First we'll add in a categorical column: the origin of the cars

```{r}
set.seed(3)
df3.auto.subset.cat <- df1.auto %>% 
    select(name,
           mpg, 
           cylinders, 
           displacement, 
           origin) %>%  
    sample_n(3)

# print: 
df3.auto.subset.cat %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")


```


\  
\  
\  


Now we'll use Gower distances on mixed data: 

```{r}
# for convenience, add rownames: 
rownames(df3.auto.subset.cat) <- df3.auto.subset.cat %>%
    pull(name) %>% 
    substr(1,10)



m2.gower.dis <- daisy(df3.auto.subset.cat, 
                      metric = "gower")

m2.gower.dis


```

As expected, this hasn't changed the similarity scores between the cars by much. 

\  
\  
\  

### Changing the data to examine the effect on Gower's measure
Let's mess with the data a bit. We'll add a lot of categorical fields that make the audi and the amc similar. Then we'll calculate Gower's dissimilarity again. 

```{r}
set.seed(3)
df4.auto.new <- df1.auto %>% 
    select(name,
           mpg, 
           cylinders, 
           displacement, 
           origin) %>%  
    sample_n(3) %>% 
    mutate(category.var.1 = as.factor(c("yes", 
                                        "yes", 
                                        "no")), 
           category.var.2 = as.factor(c("yes", 
                                        "yes", 
                                        "no")), 
           category.var.3 = as.factor(c("yes", 
                                        "yes", 
                                        "no")), 
           category.var.4 = as.factor(c("yes", 
                                        "yes", 
                                        "no")), 
           category.var.5 = as.factor(c("yes", 
                                        "yes", 
                                        "no")))






```


```{r}
# add rownames for convenience: 
rownames(df4.auto.new) <- df4.auto.new %>%
    pull(name) %>% 
    substr(1,10)


# Calculate Gower's similarity indexes: 
m3.gower.newdata <- daisy(df4.auto.new, 
                          metric = "gower")

m3.gower.newdata


```


\  
\  
\  

As expected, we have managed to make the audi and the amc the most similar. This means that Gower's measure is behaving how we would intuitively expect it to. 

\  
\  
\  

## Clustering on mixed data 
### Visualizing clusters
There are too many rows to visualize all car names in a single tree, but it's still useful to see the vertical distances between clusters.^[See http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning#plot.hclust-r-base-function] 

```{r}
# assign rownames for convenience: 
rownames.start <- df1.auto %>%
    pull(name) %>% 
    substr(1,6)

rownames <- paste(rownames.start, 
                  1:nrow(df1.auto), 
                  sep = "-")

rownames(df1.auto) <- rownames

# find distance matrix: 
m4.auto.gower.dist <- daisy(df1.auto, 
                            metric = "gower")


# now use hclust: 
c1.auto.cluster <- hclust(m4.auto.gower.dist)

# plot dendrogram: 
# Far too many rows to visualize in one tree
# plot(c1.auto.cluster, 
#      hang = -1,
#      cex = 0.6,
#      cex.lab = 0.1)

# That's not easy to interpret. Let's try an alternative plotting method: 

d1.auto.dendrogram <- as.dendrogram(c1.auto.cluster)

# Far too many rows to visualize in one tree
plot(d1.auto.dendrogram,
     horiz = FALSE, 
     leaflab = "none")


```

\  
\  
\  

### Examining split into 2 clusters

Now let's take a look at the list of cars at different levels of the dendrogram.^[See https://www.stat.berkeley.edu/~spector/s133/Clus.html] 

Given the vertical distances in the dendrogram, it looks like 2 to 4 clusters is probably most meaningful. Let's start by looking at 2 clusters. 

```{r}
groups.2 <- cutree(c1.auto.cluster, 2)

# str(groups.2) 
# this is just a numeric vector: each entry is the cluster number that the corresponding element should be assigned to. 

list1.2clusters <- sapply(unique(groups.2), 
                          function(x){df1.auto$name[groups.2 == x]})

# list1.2clusters[[1]]
# list1.2clusters[[2]]

df5.2cl.summary <- table(groups.2, 
                         df1.auto$origin) %>% 
      as.data.frame.matrix() %>% 
      mutate(cluster = c("cluster1", 
                         "cluster2")) %>% 
      select(cluster, 
             everything())

# print: 
df5.2cl.summary %>% 
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
      

```

So all the cars in Cluster 1 are American, while those in Cluster 2 are a mix of American, European and Japanese. How do these clusters differ in terms of the other variables? 

```{r}
df6.auto.numeric <- df1.auto %>% 
      select(mpg:acceleration)

aggregate(df6.auto.numeric, 
          list(groups.2), 
          median) %>% 
      
      rename(cluster.num = Group.1) %>% 
      
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
      


```

It's fairly clear that Cluster 1 is large, fuel-inefficient cars (all American), whereas Cluster 2 is all other cars. 

\  
\  
\  

### Examining split into 3 clusters 
```{r}
groups.3 <- cutree(c1.auto.cluster, 3)

# str(groups.3) 
# this is just a numeric vector: each entry is the cluster number that the corresponding element should be assigned to.

list2.3clusters <- sapply(unique(groups.3), 
                          function(x){df1.auto$name[groups.3 == x]})


df7.3cl.summary <- table(groups.3, 
                         df1.auto$origin) %>% 
      as.data.frame.matrix() %>% 
      mutate(cluster = c("cluster1", 
                         "cluster2", 
                         "cluster3")) %>% 
      select(cluster, 
             everything())

# print: 
df7.3cl.summary %>% 
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
      
# Let's examine numeric variables by cluster: 
aggregate(df6.auto.numeric, 
          list(groups.3), 
          median) %>% 
      
      rename(cluster.num = Group.1) %>% 
      
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")



```

Very interesting. Cluster 1 is the large American cars, Cluster 3 is European Cars, and Cluster 2 is Japanese cars and Japanese-like American cars. 

Additionally, note that Clusters 2 and 3 are very similar, but Cluster 3 seems to have slightly smaller, more fuel-efficient cars. 

\  
\  
\  

### Examining split into 4 clusters 
```{r}
groups.4 <- cutree(c1.auto.cluster, 4)

# str(groups.4) 
# this is just a numeric vector: each entry is the cluster number that the corresponding element should be assigned to.

list3.4clusters <- sapply(unique(groups.4), 
                          function(x){df1.auto$name[groups.4 == x]})


df8.4cl.summary <- table(groups.4, 
                         df1.auto$origin) %>% 
      as.data.frame.matrix() %>% 
      mutate(cluster = c("cluster1", 
                         "cluster2", 
                         "cluster3", 
                         "cluster4")) %>% 
      select(cluster, 
             everything())

# print: 
df8.4cl.summary %>% 
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
      
# Let's examine numeric variables by cluster: 
aggregate(df6.auto.numeric, 
          list(groups.4), 
          median) %>% 
      
      rename(cluster.num = Group.1) %>% 
      
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")



```

At this point, we've managed to split apart the Japanese cars from the Japanese-like American cars. 

Finally, just for fun, let's look at the breakdown into 5 clusters. 

\  
\  
\  

### Examining split into 5 clusters 
```{r}
groups.4 <- cutree(c1.auto.cluster, 4)

# str(groups.4) 
# this is just a numeric vector: each entry is the cluster number that the corresponding element should be assigned to.

list3.4clusters <- sapply(unique(groups.4), 
                          function(x){df1.auto$name[groups.4 == x]})


df8.4cl.summary <- table(groups.4, 
                         df1.auto$origin) %>% 
      as.data.frame.matrix() %>% 
      mutate(cluster = c("cluster1", 
                         "cluster2", 
                         "cluster3", 
                         "cluster4")) %>% 
      select(cluster, 
             everything())

# print: 
df8.4cl.summary %>% 
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
      
# Let's examine numeric variables by cluster: 
aggregate(df6.auto.numeric, 
          list(groups.4), 
          median) %>% 
      
      rename(cluster.num = Group.1) %>% 
      
      kable() %>% 
      kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")



```



\  
\  
\  

## References
